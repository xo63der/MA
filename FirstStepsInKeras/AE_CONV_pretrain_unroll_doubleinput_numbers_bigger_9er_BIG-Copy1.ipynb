{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.1846 - model_5_loss: 0.0835 - val_loss: 0.1188 - val_model_5_loss: 0.0319\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.1152 - model_5_loss: 0.0299 - val_loss: 0.1103 - val_model_5_loss: 0.0255\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.1099 - model_5_loss: 0.0266 - val_loss: 0.1071 - val_model_5_loss: 0.0243\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.1079 - model_5_loss: 0.0253 - val_loss: 0.1077 - val_model_5_loss: 0.0229\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.1064 - model_5_loss: 0.0244 - val_loss: 0.1039 - val_model_5_loss: 0.0234\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.1052 - model_5_loss: 0.0238 - val_loss: 0.1050 - val_model_5_loss: 0.0222\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.1045 - model_5_loss: 0.0233 - val_loss: 0.1024 - val_model_5_loss: 0.0230\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.1034 - model_5_loss: 0.0228 - val_loss: 0.1019 - val_model_5_loss: 0.0229\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.1033 - model_5_loss: 0.0225 - val_loss: 0.1046 - val_model_5_loss: 0.0214\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.1027 - model_5_loss: 0.0222 - val_loss: 0.1031 - val_model_5_loss: 0.0213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f62a2978940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# convolutional autoencoder in keras\n",
    "\n",
    "import os\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.datasets import mnist\n",
    "from keras.regularizers import l1\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "from keras.datasets import mnist \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def own_kullback_leibler_divergence(y_true, y_pred):\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "    return K.sum(y_true * K.log(y_true / y_pred), axis=-1)\n",
    "\n",
    "def own_mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "def own_loss(y_true, y_pred):\n",
    "    return 1.0*own_kullback_leibler_divergence(y_true, y_pred)+own_mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# utility function for showing images\n",
    "def show_imgs(x_test, decoded_imgs, n=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        if decoded_imgs is not None:\n",
    "            ax = plt.subplot(2, n, i+ 1 +n)\n",
    "            plt.imshow(decoded_imgs[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "### Create input\n",
    "    \n",
    "input1_img = Input(shape=(28, 28,1)) # 1ch=black&white, 28 x 28\n",
    "input2_img = Input(shape=(28, 28,1)) # 1ch=black&white, 28 x 28\n",
    "\n",
    "(x_all, y_all), (x_all_test, y_all_test) = mnist.load_data()\n",
    "\n",
    "x_all = x_all.astype('float32')/255. # 0-1.に変換\n",
    "x_all_test = x_all_test.astype('float32')/255.\n",
    "\n",
    "###\n",
    "\n",
    "x_9_list=[]\n",
    "for i in range(len(y_all)):\n",
    "    if (y_all[i] == 9):\n",
    "        x_9_list.append(x_all[i])\n",
    "x_9=np.asarray(x_9_list[:5000])\n",
    "x_9_list_test=[]\n",
    "for i in range(len(y_all_test)):\n",
    "    if (y_all_test[i] == 9):\n",
    "        x_9_list_test.append(x_all_test[i])\n",
    "x_9_test=np.asarray(x_9_list_test[:500])\n",
    "\n",
    "###\n",
    "\n",
    "x_6_list=[]\n",
    "for i in range(len(y_all)):\n",
    "    if (y_all[i] == 6):\n",
    "        x_6_list.append(x_all[i])\n",
    "x_6=np.asarray(x_6_list[:5000])\n",
    "x_6_list_test=[]\n",
    "for i in range(len(y_all_test)):\n",
    "    if (y_all_test[i] == 6):\n",
    "        x_6_list_test.append(x_all_test[i])\n",
    "x_6_test=np.asarray(x_6_list_test[:500])\n",
    "\n",
    "###\n",
    "\n",
    "x_8_list=[]\n",
    "for i in range(len(y_all)):\n",
    "    if (y_all[i] == 8):\n",
    "        x_8_list.append(x_all[i])\n",
    "x_8=np.asarray(x_8_list[:5000])\n",
    "x_8_list_test=[]\n",
    "for i in range(len(y_all_test)):\n",
    "    if (y_all_test[i] == 8):\n",
    "        x_8_list_test.append(x_all_test[i])\n",
    "x_8_test=np.asarray(x_8_list_test[:500])\n",
    "\n",
    "###\n",
    "\n",
    "x_train=x_9\n",
    "x_test=x_9_test\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "x_train_mask = x_train - x_train * np.random.randint(2, size=x_train.shape)\n",
    "x_test_mask = x_test - x_test * np.random.randint(2, size=x_test.shape)\n",
    "\n",
    "\n",
    "input_img = Input(shape=(28, 28,1)) # 1ch=black&white, 28 x 28\n",
    "input1 = Input(shape=(28, 28,1))\n",
    "input2 = Input(shape=(28, 28,1))\n",
    "input_pool1 = MaxPooling2D(pool_size=(2, 2), name='i1p1')(input_img)\n",
    "input_conv  = Conv2D(16, (3, 3), activation='relu', padding='same', name='i1c1')(input_pool1)\n",
    "input_pool2 = UpSampling2D(size=(2, 2), name='i1p4')(input_conv)\n",
    "decoded = Conv2D(1, (5, 5), activation='sigmoid', padding='same', name='i2d1')(input_pool2)\n",
    "\n",
    "model = Model(inputs=input_img, outputs=decoded)\n",
    "\n",
    "decoded1 = model(input1)\n",
    "decoded2 = model(input2)\n",
    "\n",
    "siam = Model(inputs=[input1,input2], outputs=[decoded1,decoded2])\n",
    "siam.compile(optimizer='adadelta', loss=['mean_squared_error','mean_squared_error'], loss_weights=[1.0,1.0])\n",
    "siam.fit([x_train_noisy[:2500], x_train_mask[2500:]], [x_train_noisy[:2500], x_train[2500:]], epochs=10, batch_size=20,\n",
    "            shuffle=True, validation_data=([x_test_noisy, x_test_mask], [x_test_noisy, x_test]), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 9s 2ms/step - loss: 0.0333 - val_loss: 0.0037\n",
      "Epoch 2/10\n",
      " 100/5000 [..............................] - ETA: 8s - loss: 0.0037"
     ]
    }
   ],
   "source": [
    "### pretrain outer layers\n",
    "\n",
    "input1_train1_conv1 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i1t1c1')(input1_img) #nb_filter, nb_row, nb_col\n",
    "input1_train1_decoded = Conv2D(1, (5, 5), activation='sigmoid', padding='same', name='i1t1d1')(input1_train1_conv1)\n",
    "\n",
    "input2_train1_conv1 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i2t1c1')(input2_img) #nb_filter, nb_row, nb_col\n",
    "input2_train1_decoded = Conv2D(1, (5, 5), activation='sigmoid', padding='same', name='i2t1d1')(input2_train1_conv1)\n",
    "\n",
    "input1_train1_autoencoder = Model(input1_img, input1_train1_decoded)\n",
    "input1_train1_autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "\n",
    "input2_train1_autoencoder = Model(input2_img, input2_train1_decoded)\n",
    "input2_train1_autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "\n",
    "input1_train1_autoencoder.fit(x_train_noisy, x_train_noisy, epochs=10, batch_size=20,\n",
    "               shuffle=True, validation_data=(x_test_noisy, x_test_noisy), verbose=1)\n",
    "\n",
    "input2_train1_autoencoder.fit(x_train_mask, x_train, epochs=10, batch_size=20,\n",
    "               shuffle=True, validation_data=(x_test_mask, x_test), verbose=1)\n",
    "\n",
    "# create input for inner pretrain\n",
    "\n",
    "input1_creator_train2 = Model(input1_img, input1_train1_conv1)\n",
    "for i in range(2):\n",
    "    input1_creator_train2.layers[i].set_weights(input1_train1_autoencoder.layers[i].get_weights())\n",
    "    \n",
    "input2_creator_train2 = Model(input2_img, input2_train1_conv1)\n",
    "for i in range(2):\n",
    "    input2_creator_train2.layers[i].set_weights(input2_train1_autoencoder.layers[i].get_weights())\n",
    "\n",
    "input1_train2 = input1_creator_train2.predict(x_train_noisy)\n",
    "input2_train2 = input2_creator_train2.predict(x_train_mask)\n",
    "\n",
    "print(input1_train2.shape)\n",
    "print(input2_train2.shape)\n",
    "\n",
    "# inner pretrain\n",
    "\n",
    "input1_img_train2 = Input(shape=(28, 28,16))\n",
    "input1_train2_pool1 = MaxPooling2D(pool_size=(2, 2), name='i1t2p1')(input1_img_train2)\n",
    "input1_train2_conv1 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i1t2c1')(input1_train2_pool1) #nb_filter, nb_row, nb_col\n",
    "input1_train2_pool2 = UpSampling2D(size=(2, 2), name='i1t2p2')(input1_train2_conv1)\n",
    "input1_train2_decoded = Conv2D(16, (3, 3), activation='relu', padding='same', name='i1t2d1')(input1_train2_pool2)\n",
    "\n",
    "input2_img_train2 = Input(shape=(28, 28,16))\n",
    "input2_train2_pool1 = MaxPooling2D(pool_size=(2, 2), name='i2t2p1')(input2_img_train2)\n",
    "input2_train2_conv1 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i2t2c1')(input2_train2_pool1) #nb_filter, nb_row, nb_col\n",
    "input2_train2_pool2 = UpSampling2D(size=(2, 2), name='i2t2p2')(input2_train2_conv1)\n",
    "input2_train2_decoded = Conv2D(16, (3, 3), activation='relu', padding='same', name='i2t2d1')(input2_train2_pool2)\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "input1_train2_autoencoder = Model(input1_img_train2, input1_train2_decoded)\n",
    "input1_train2_autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "\n",
    "input2_train2_autoencoder = Model(input2_img_train2, input2_train2_decoded)\n",
    "input2_train2_autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "\n",
    "input1_train2_autoencoder.fit(input1_train2, input1_train2, epochs=10, batch_size=20, verbose=1)\n",
    "input2_train2_autoencoder.fit(input2_train2, input2_train2, epochs=10, batch_size=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input for final pretrain\n",
    "\n",
    "input1_creator_train3 = Model(input1_img_train2, input1_train2_conv1)\n",
    "for i in range(3):\n",
    "    input1_creator_train3.layers[i].set_weights(input1_train2_autoencoder.layers[i].get_weights())\n",
    "\n",
    "input1_train3 = input1_creator_train3.predict(input1_creator_train2.predict(x_train_noisy))\n",
    "\n",
    "print(input1_train3.shape)\n",
    "\n",
    "input2_creator_train3 = Model(input2_img_train2, input2_train2_conv1)\n",
    "for i in range(3):\n",
    "    input2_creator_train3.layers[i].set_weights(input1_train2_autoencoder.layers[i].get_weights())\n",
    "\n",
    "input2_train3 = input2_creator_train3.predict(input2_creator_train2.predict(x_train_mask))\n",
    "\n",
    "print(input2_train3.shape)\n",
    "\n",
    "# final pretrain\n",
    "\n",
    "input1_img_train3 = Input(shape=(14, 14,16))\n",
    "input2_img_train3 = Input(shape=(14, 14,16))\n",
    "\n",
    "from keras.layers import average\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "input1_norm=BatchNormalization(axis=-1, name='trainnorm1')(input1_img_train3)\n",
    "input2_norm=BatchNormalization(axis=-1, name='trainnorm2')(input2_img_train3)\n",
    "\n",
    "combine = average([input1_norm,input2_norm], name='t3combine')\n",
    "#train3_pool1 = MaxPooling2D(pool_size=(2, 2), name = 't3combinep1')(combine)\n",
    "train3_conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', name = 't3combinec1')(combine) #nb_filter, nb_row, nb_col\n",
    "\n",
    "#---\n",
    "\n",
    "#train3_pool2 = UpSampling2D(size=(2, 2), name = 't3combinep2')(train3_conv1)\n",
    "\n",
    "input1_train3_decoded = Conv2D(16, (3, 3), activation='relu', padding='same', name = 't3i1c2')(train3_conv1)\n",
    "input2_train3_decoded = Conv2D(16, (3, 3), activation='relu', padding='same', name = 't3i2c2')(train3_conv1)\n",
    "\n",
    "train3_autoencoder = Model(inputs=[input1_img_train3,input2_img_train3], outputs=[input1_train3_decoded,input2_train3_decoded])\n",
    "train3_autoencoder.compile(optimizer='adadelta', loss=['mean_squared_error','mean_squared_error'], loss_weights=[1.0,1.0])\n",
    "\n",
    "train3_autoencoder.fit([input1_train3, input2_train3], [input1_train3, input2_train3], epochs=10, batch_size=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### unroll\n",
    "\n",
    "input1_img = Input(shape=(28, 28,1)) # 1ch=black&white, 28 x 28\n",
    "input2_img = Input(shape=(28, 28,1)) # 1ch=black&white, 28 x 28\n",
    "\n",
    "input1_conv1 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i1c1')(input1_img) #nb_filter, nb_row, nb_col\n",
    "\n",
    "input1_pool1 = MaxPooling2D(pool_size=(2, 2), name='i1p1')(input1_conv1)\n",
    "input1_conv2 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i1c2')(input1_pool1)\n",
    "\n",
    "input2_conv1 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i2c1')(input2_img) #nb_filter, nb_row, nb_col\n",
    "\n",
    "input2_pool1 = MaxPooling2D(pool_size=(2, 2), name='i2p1')(input2_conv1)\n",
    "input2_conv2 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i2c2')(input2_pool1)\n",
    "\n",
    "input1_norm=BatchNormalization(axis=-1, name='norm1')(input1_conv2)\n",
    "input2_norm=BatchNormalization(axis=-1, name='norm2')(input2_conv2)\n",
    "\n",
    "combine = average([input1_norm,input2_norm], name='combine')\n",
    "#combine_pool2 = MaxPooling2D(pool_size=(2, 2), name='combinepool2')(combine)\n",
    "\n",
    "encoded = Conv2D(32, (3, 3), activation='relu', padding='same', name='encode')(combine)\n",
    "\n",
    "#combine_pool3 = UpSampling2D(size=(2, 2), name='combinepool3')(encoded)\n",
    "\n",
    "input1_conv4 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i1c4')(encoded)\n",
    "\n",
    "input1_pool4 = UpSampling2D(size=(2, 2), name='i1p4')(input1_conv4)\n",
    "input1_conv5 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i1c5')(input1_pool4)\n",
    "\n",
    "input1_decoded = Conv2D(1, (5, 5), activation='sigmoid', padding='same', name='i1d1')(input1_conv5)\n",
    "\n",
    "input2_conv4 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i2c4')(encoded)\n",
    "\n",
    "input2_pool4 = UpSampling2D(size=(2, 2), name='i2p4')(input2_conv4)\n",
    "input2_conv5 = Conv2D(16, (3, 3), activation='relu', padding='same', name='i2c5')(input2_pool4)\n",
    "\n",
    "input2_decoded = Conv2D(1, (5, 5), activation='sigmoid', padding='same', name='i2d1')(input2_conv5)\n",
    "\n",
    "autoencoder = Model(inputs=[input1_img,input2_img], outputs=[input1_decoded,input2_decoded])\n",
    "autoencoder.compile(optimizer='adadelta', loss=['mean_squared_error','mean_squared_error'], loss_weights=[1.0,1.0])\n",
    "\n",
    "names = [weight.name for layer in autoencoder.layers for weight in layer.weights]\n",
    "weights = autoencoder.get_weights()\n",
    "\n",
    "for name, weight in zip(names, weights):\n",
    "    print(name, weight.shape)\n",
    "    \n",
    "# initialize\n",
    "\n",
    "autoencoder.get_layer(\"i1c1\").set_weights(input1_train1_autoencoder.get_layer('i1t1c1').get_weights())\n",
    "autoencoder.get_layer(\"i2c1\").set_weights(input2_train1_autoencoder.get_layer('i2t1c1').get_weights())\n",
    "autoencoder.get_layer(\"i1d1\").set_weights(input1_train1_autoencoder.get_layer('i1t1d1').get_weights())\n",
    "autoencoder.get_layer(\"i2d1\").set_weights(input2_train1_autoencoder.get_layer('i2t1d1').get_weights())\n",
    "\n",
    "#---\n",
    "\n",
    "#autoencoder.get_layer(\"i1p1\").set_weights(input1_train2_autoencoder.get_layer('i1t2p1').get_weights())\n",
    "#autoencoder.get_layer(\"i2p1\").set_weights(input2_train2_autoencoder.get_layer('i2t2p1').get_weights())\n",
    "\n",
    "autoencoder.get_layer(\"i1c2\").set_weights(input1_train2_autoencoder.get_layer('i1t2c1').get_weights())\n",
    "autoencoder.get_layer(\"i2c2\").set_weights(input2_train2_autoencoder.get_layer('i2t2c1').get_weights())\n",
    "\n",
    "#autoencoder.get_layer(\"i1p4\").set_weights(input1_train2_autoencoder.get_layer('i1t2p2').get_weights())\n",
    "#autoencoder.get_layer(\"i2p4\").set_weights(input2_train2_autoencoder.get_layer('i2t2p2').get_weights())\n",
    "\n",
    "autoencoder.get_layer(\"i1c5\").set_weights(input1_train2_autoencoder.get_layer('i1t2d1').get_weights())\n",
    "autoencoder.get_layer(\"i2c5\").set_weights(input2_train2_autoencoder.get_layer('i2t2d1').get_weights())\n",
    "\n",
    "#---\n",
    "\n",
    "#autoencoder.get_layer(\"combinepool2\").set_weights(train3_autoencoder.get_layer('t3combinep1').get_weights())\n",
    "\n",
    "#autoencoder.get_layer(\"combinepool3\").set_weights(train3_autoencoder.get_layer('t3combinep2').get_weights())\n",
    "\n",
    "autoencoder.get_layer(\"encode\").set_weights(train3_autoencoder.get_layer('t3combinec1').get_weights())\n",
    "\n",
    "autoencoder.get_layer(\"norm1\").set_weights(train3_autoencoder.get_layer('trainnorm1').get_weights())\n",
    "autoencoder.get_layer(\"norm2\").set_weights(train3_autoencoder.get_layer('trainnorm2').get_weights())\n",
    "\n",
    "autoencoder.get_layer(\"i1c4\").set_weights(train3_autoencoder.get_layer('t3i1c2').get_weights())\n",
    "autoencoder.get_layer(\"i2c4\").set_weights(train3_autoencoder.get_layer('t3i2c2').get_weights())\n",
    "\n",
    "### fine tune\n",
    "\n",
    "autoencoder.fit([x_train_noisy, x_train_mask], [x_train_noisy, x_train], epochs=10, batch_size=20,\n",
    "            shuffle=True, validation_data=([x_test_noisy, x_test_mask], [x_test_noisy, x_test]), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(x_test, decoded_imgs, n=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(4, n, i+1)\n",
    "        plt.imshow(x_test[0][i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "        ax = plt.subplot(4, n, i+1 +n)\n",
    "        plt.imshow(x_test[1][i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        ax = plt.subplot(4, n, i+ 1 +2*n)\n",
    "        plt.imshow(decoded_imgs[0][i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "        ax = plt.subplot(4, n, i+ 1 +3*n)\n",
    "        plt.imshow(decoded_imgs[1][i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "decoded_imgs = autoencoder.predict([x_test_noisy, x_test_mask])\n",
    "show_imgs([x_test_noisy, x_test_mask], decoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enbig1 = UpSampling2D(size=(2, 2))(encoded)\n",
    "#enbig2 = UpSampling2D(size=(2, 2))(enbig1)\n",
    "\n",
    "autoencoder_show_hidden = Model([input1_img, input2_img], enbig1)\n",
    "encoded_imgs = autoencoder_show_hidden.predict([x_test_noisy, x_test_mask])\n",
    "\n",
    "print(encoded_imgs.shape)\n",
    "\n",
    "def show_hidden(x_test, encoded_imgs, n=10):\n",
    "    \n",
    "    en0=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "    en1=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "    en2=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "    en3=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "\n",
    "    for i in range(encoded_imgs.shape[0]):\n",
    "        for j in range(encoded_imgs.shape[1]):\n",
    "            for k in range(encoded_imgs.shape[2]):\n",
    "                en0[i][j][k]=encoded_imgs[i][j][k][0]\n",
    "                en1[i][j][k]=encoded_imgs[i][j][k][1]\n",
    "                en2[i][j][k]=encoded_imgs[i][j][k][2]\n",
    "                en3[i][j][k]=encoded_imgs[i][j][k][3]\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(5, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        if decoded_imgs is not None:\n",
    "            ax = plt.subplot(5, n, i+ 1 +n)\n",
    "            plt.imshow(en0[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "            ax = plt.subplot(5, n, i+ 1 +2*n)\n",
    "            plt.imshow(en1[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "            ax = plt.subplot(5, n, i+ 1 +3*n)\n",
    "            plt.imshow(en2[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "            ax = plt.subplot(5, n, i+ 1 +4*n)\n",
    "            plt.imshow(en3[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "show_hidden(x_test_noisy, encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input1_train1_autoencoder.get_layer(\"i1t1c1\").get_weights()[0]==input2_train1_autoencoder.get_layer(\"i2t1c1\").get_weights()[0])\n",
    "print(input1_train1_autoencoder.get_layer(\"i1t1c1\").get_weights()[1]==input2_train1_autoencoder.get_layer(\"i2t1c1\").get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_train1_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2_train1_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_train2_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2_train2_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autoencoder.get_layer(\"i1c1\").get_weights()[0][0][0]==autoencoder.get_layer(\"i2c1\").get_weights()[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2] [3, 4, 5, 6, 7, 8, 9, 0] [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3,4,5,6,7,8,9,0]\n",
    "\n",
    "print(a[:2],a[2:],a[:-2])\n",
    "print(a[2:]+a[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
