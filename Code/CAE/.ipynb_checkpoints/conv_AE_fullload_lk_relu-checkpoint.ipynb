{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoffmann/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n"
     ]
    }
   ],
   "source": [
    "###### %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# convolutional autoencoder in keras\n",
    "\n",
    "import os\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "from keras.datasets import mnist \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# utility function for showing images\n",
    "def show_imgs(x_test, decoded_imgs, n=4):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(size,size,3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        if decoded_imgs is not None:\n",
    "            ax = plt.subplot(2, n, i+ 1 +n)\n",
    "            plt.imshow(decoded_imgs[i].reshape(size,size,3))\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    \n",
    "def show_imgs_gray_out(x_test, decoded_imgs, n=4):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(size,size,3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        if decoded_imgs is not None:\n",
    "            ax = plt.subplot(2, n, i+ 1 +n)\n",
    "            plt.imshow(decoded_imgs[i].reshape(size,size))\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    \n",
    "def show_hidden(x_test, encoded_imgs, n=10):\n",
    "    \n",
    "    en0=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "    en1=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "    en2=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "    en3=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "\n",
    "    for i in range(encoded_imgs.shape[0]):\n",
    "        for j in range(encoded_imgs.shape[1]):\n",
    "            for k in range(encoded_imgs.shape[2]):\n",
    "                en0[i][j][k]=encoded_imgs[i][j][k][0]\n",
    "                en1[i][j][k]=encoded_imgs[i][j][k][1]\n",
    "                en2[i][j][k]=encoded_imgs[i][j][k][2]\n",
    "                en3[i][j][k]=encoded_imgs[i][j][k][3]\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(5, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(size,size,3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        if decoded_imgs is not None:\n",
    "            ax = plt.subplot(5, n, i+ 1 +n)\n",
    "            plt.imshow(en0[i].reshape(int(size/4),int(size/4)))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "            ax = plt.subplot(5, n, i+ 1 +2*n)\n",
    "            plt.imshow(en1[i].reshape(int(size/4),int(size/4)))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "            ax = plt.subplot(5, n, i+ 1 +3*n)\n",
    "            plt.imshow(en2[i].reshape(int(size/4),int(size/4)))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "            ax = plt.subplot(5, n, i+ 1 +4*n)\n",
    "            plt.imshow(en3[i].reshape(int(size/4),int(size/4)))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "def getMaxGrads(input_set, model, layer_name, nof):\n",
    "    \n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    \n",
    "    out = np.zeros((len(input_set),len(input_set[0]),len(input_set[0][0])))\n",
    "\n",
    "    for i in range(nof):\n",
    "        filter_index = i  # can be any integer from 0 to 511, as there are 512 filters in that layer\n",
    "\n",
    "        layer_output = layer_dict[layer_name].output\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "        # compute the gradient of the input picture wrt this loss\n",
    "        grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "        #normalization trick: we normalize the gradient\n",
    "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)  ### normalize? ... later\n",
    "\n",
    "        # this function returns the loss and grads given the input picture\n",
    "        iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "        it = iterate([input_set])\n",
    "        \n",
    "        len1=len(out)\n",
    "        len2=len(out[0])\n",
    "        len3=len(out[0][0])\n",
    "        \n",
    "        for j in range(len1):\n",
    "            for k in range(len2):\n",
    "                for l in range(len3):\n",
    "                    if np.mean(abs(it[1][j][k][l])) > out[j][k][l]:   #abs()?\n",
    "                        out[j][k][l]=np.mean(abs(it[1][j][k][l]))\n",
    "                        \n",
    "        return out\n",
    "    \n",
    "def lk_relu(x):\n",
    "    return K.relu(x, alpha=0.3)\n",
    "\n",
    "print(\"Starting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data done\n",
      "Test Data done\n"
     ]
    }
   ],
   "source": [
    "### Create input\n",
    "\n",
    "size=256\n",
    "\n",
    "train_path = \"/home/hoffmann/MA/OIRDS_v1_0/train/train/\"\n",
    "test_path = \"/home/hoffmann/MA/OIRDS_v1_0/test/test/\"\n",
    "\n",
    "train_batches = []\n",
    "listing = os.listdir(train_path) \n",
    "for file in listing:\n",
    "    im = Image.open(train_path + file)   \n",
    "    im_rsz = im.resize((size,size))\n",
    "    train_batches.append(np.asarray(im_rsz, dtype=\"float64\") / 255)\n",
    "train_batches = np.asarray(train_batches)\n",
    " \n",
    "    \n",
    "print(\"Train Data done\")\n",
    "\n",
    "test_batches = []\n",
    "listing = os.listdir(test_path) \n",
    "\n",
    "for file in listing:\n",
    "    im = Image.open(test_path + file)   \n",
    "    im_rsz = im.resize((size,size))\n",
    "    test_batches.append(np.asarray(im_rsz, dtype=\"float64\") / 255)\n",
    "test_batches = np.asarray(test_batches)\n",
    "    \n",
    "print(\"Test Data done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 845 samples, validate on 37 samples\n",
      "Epoch 1/1\n",
      "845/845 [==============================] - 114s 135ms/step - loss: 0.2634 - val_loss: 0.1156\n",
      "(845, 256, 256, 32)\n",
      "Epoch 1/1\n",
      "845/845 [==============================] - 145s 172ms/step - loss: 0.8748\n",
      "(845, 128, 128, 32)\n",
      "Epoch 1/1\n",
      "845/845 [==============================] - 33s 39ms/step - loss: 0.8414\n",
      "(845, 64, 64, 16)\n",
      "Epoch 1/1\n",
      "845/845 [==============================] - 4s 4ms/step - loss: 0.8394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f553e1b4c50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxfil = 32\n",
    "bs_z=16\n",
    "\n",
    "### pretrain1\n",
    "    \n",
    "input_img = Input(shape=(size, size,3))\n",
    "\n",
    "train1_norm1=BatchNormalization(axis=-1, name='t1n1')(input_img)\n",
    "train1_c1 = Conv2D(maxfil, (3, 3), activation=lk_relu, padding='same', name='t1c1')(train1_norm1)\n",
    "train1_norm2=BatchNormalization(axis=-1, name='t1n2')(train1_c1)\n",
    "train1_decoded = Conv2DTranspose(3, (3, 3), activation=lk_relu, padding='same', name='t1tc1')(train1_norm2)\n",
    "\n",
    "train1_autoencoder = Model(input_img, train1_decoded)\n",
    "train1_autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "\n",
    "train1_autoencoder.fit(train_batches, train_batches, epochs=1, validation_data=(test_batches, test_batches), batch_size=bs_z, verbose=1)\n",
    "\n",
    "### pretrain2\n",
    "\n",
    "creator_train2 = Model(input_img, train1_norm2)\n",
    "creator_train2.get_layer(\"t1n1\").set_weights(train1_autoencoder.get_layer('t1n1').get_weights())\n",
    "creator_train2.get_layer(\"t1c1\").set_weights(train1_autoencoder.get_layer('t1c1').get_weights())\n",
    "creator_train2.get_layer(\"t1n2\").set_weights(train1_autoencoder.get_layer('t1n2').get_weights())\n",
    "\n",
    "input_train2 = creator_train2.predict(train_batches)\n",
    "\n",
    "print(input_train2.shape)\n",
    "\n",
    "#---\n",
    "\n",
    "input_img_train2 = Input(shape=(size, size, maxfil))\n",
    "train2_p1 = MaxPooling2D(pool_size=(2, 2), name='t2p1')(input_img_train2)\n",
    "train2_c1 = Conv2D(maxfil, (3, 3), activation=lk_relu, padding='same', name='t2c1')(train2_p1)\n",
    "train2_norm1=BatchNormalization(axis=-1, name='t2n1')(train2_c1)\n",
    "train2_tc1 = Conv2DTranspose(maxfil, (3, 3), activation=lk_relu, padding='same', name='t2tc1')(train2_norm1)\n",
    "train2_norm2=BatchNormalization(axis=-1, name='t2n2')(train2_tc1)\n",
    "train2_decoded = UpSampling2D(size=(2, 2), name='t3u1')(train2_norm2)\n",
    "\n",
    "train2_autoencoder = Model(input_img_train2, train2_decoded)\n",
    "train2_autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "\n",
    "train2_autoencoder.fit(input_train2, input_train2, epochs=1, batch_size=bs_z, verbose=1)\n",
    "\n",
    "### pretrain3\n",
    "\n",
    "creator_train3 = Model(input_img_train2, train2_norm1)\n",
    "creator_train3.get_layer(\"t2c1\").set_weights(train2_autoencoder.get_layer('t2c1').get_weights())\n",
    "creator_train3.get_layer(\"t2n1\").set_weights(train2_autoencoder.get_layer('t2n1').get_weights())\n",
    "\n",
    "input_train3 = creator_train3.predict(input_train2)\n",
    "\n",
    "print(input_train3.shape)\n",
    "\n",
    "#---\n",
    "\n",
    "input_img_train3 = Input(shape=(size/2, size/2, maxfil))\n",
    "train3_p1 = MaxPooling2D(pool_size=(2, 2), name='t3p1')(input_img_train3)\n",
    "train3_c1 = Conv2D(int(maxfil/2), (3, 3), activation=lk_relu, padding='same', name='t3c1')(train3_p1)\n",
    "train3_norm1=BatchNormalization(axis=-1, name='t3n1')(train3_c1)\n",
    "train3_tc1 = Conv2DTranspose(maxfil, (3, 3), activation=lk_relu, padding='same', name='t3tc1')(train3_norm1)\n",
    "train3_norm2=BatchNormalization(axis=-1, name='t3n2')(train3_tc1)\n",
    "train3_decoded = UpSampling2D(size=(2, 2), name='t3u1')(train3_norm2)\n",
    "\n",
    "train3_autoencoder = Model(input_img_train3, train3_decoded)\n",
    "train3_autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "\n",
    "train3_autoencoder.fit(input_train3, input_train3, epochs=1, batch_size=bs_z, verbose=1)\n",
    "\n",
    "#---\n",
    "\n",
    "### pretrain3\n",
    "\n",
    "creator_train4 = Model(input_img_train3, train3_norm1)\n",
    "creator_train4.get_layer(\"t3c1\").set_weights(train3_autoencoder.get_layer('t3c1').get_weights())\n",
    "creator_train4.get_layer(\"t3n1\").set_weights(train3_autoencoder.get_layer('t3n1').get_weights())\n",
    "\n",
    "input_train4 = creator_train4.predict(input_train3)\n",
    "\n",
    "print(input_train4.shape)\n",
    "\n",
    "input_img_train4 = Input(shape=(size/4, size/4, int(maxfil/2)))\n",
    "train4_p1 = MaxPooling2D(pool_size=(2, 2), name='t4p1')(input_img_train4)\n",
    "train4_c1 = Conv2D(int(maxfil/4), (3, 3), activation=lk_relu, padding='same', name='t4c1')(train4_p1)\n",
    "train4_norm1=BatchNormalization(axis=-1, name='t4n1')(train4_c1)\n",
    "train4_tc1 = Conv2DTranspose(int(maxfil/2), (3, 3), activation=lk_relu, padding='same', name='t4tc1')(train4_norm1)\n",
    "train4_norm2=BatchNormalization(axis=-1, name='t4n2')(train4_tc1)\n",
    "train4_decoded = UpSampling2D(size=(2, 2), name='t4u1')(train4_norm2)\n",
    "\n",
    "train4_autoencoder = Model(input_img_train4, train4_decoded)\n",
    "train4_autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "\n",
    "train4_autoencoder.fit(input_train4, input_train4, epochs=1, batch_size=bs_z, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### unroll\n",
    "\n",
    "input_img = Input(shape=(size, size,3))\n",
    "\n",
    "n0 = BatchNormalization(axis=-1, name='n0', weights=train1_autoencoder.get_layer(\"t1n1\").get_weights())(input_img)\n",
    "c1 = Conv2D(maxfil, (3, 3), activation=lk_relu, padding='same', name='c1', weights=train1_autoencoder.get_layer(\"t1c1\").get_weights())(n0)\n",
    "n1 = BatchNormalization(axis=-1, name='n1', weights=train1_autoencoder.get_layer(\"t1n2\").get_weights())(c1)\n",
    "p1 = MaxPooling2D(pool_size=(2, 2), name='p1')(n1)\n",
    "c2 = Conv2D(maxfil, (3, 3), activation=lk_relu, padding='same', name='c2', weights=train2_autoencoder.get_layer(\"t2c1\").get_weights())(p1)\n",
    "n2 = BatchNormalization(axis=-1, name='n2', weights=train2_autoencoder.get_layer(\"t2n1\").get_weights())(c2)\n",
    "p2 = MaxPooling2D(pool_size=(2, 2), name='p2')(n2)\n",
    "c3 = Conv2D(int(maxfil/2), (3, 3), activation=lk_relu, padding='same', name='c3', weights=train3_autoencoder.get_layer(\"t3c1\").get_weights())(p2)\n",
    "n3 = BatchNormalization(axis=-1, name='n3', weights=train3_autoencoder.get_layer(\"t3n1\").get_weights())(c3)\n",
    "p3 = MaxPooling2D(pool_size=(2, 2), name='p3')(n3)\n",
    "c4 = Conv2D(int(maxfil/4), (3, 3), activation=lk_relu, padding='same', name='c4', weights=train4_autoencoder.get_layer(\"t4c1\").get_weights())(p3)\n",
    "encoded = BatchNormalization(axis=-1, name='n4', weights=train4_autoencoder.get_layer(\"t4n1\").get_weights())(c4)\n",
    "tc4 = Conv2DTranspose(int(maxfil/2), (3, 3), activation=lk_relu, padding='same', name='tc4', weights=train4_autoencoder.get_layer(\"t4tc1\").get_weights())(encoded)\n",
    "n4 = BatchNormalization(axis=-1, name='n4', weights=train4_autoencoder.get_layer(\"t4n2\").get_weights())(tc4)\n",
    "u3 = UpSampling2D(size=(2, 2), name='u3')(n4)\n",
    "tc3 = Conv2DTranspose(maxfil, (3, 3), activation=lk_relu, padding='same', name='tc3', weights=train3_autoencoder.get_layer(\"t3tc1\").get_weights())(n3)\n",
    "n5 = BatchNormalization(axis=-1, name='n5', weights=train3_autoencoder.get_layer(\"t3n2\").get_weights())(tc3)\n",
    "u2 = UpSampling2D(size=(2, 2), name='u2')(n5)\n",
    "tc2 = Conv2DTranspose(maxfil, (3, 3), activation=lk_relu, padding='same', name='tc2', weights=train2_autoencoder.get_layer(\"t2tc1\").get_weights())(u2)\n",
    "n6 = BatchNormalization(axis=-1, name='n6', weights=train2_autoencoder.get_layer(\"t2n2\").get_weights())(tc2)\n",
    "u1 = UpSampling2D(size=(2, 2), name='u1')(n6)\n",
    "decoded = Conv2DTranspose(3, (3, 3), activation=lk_relu, padding='same', name='tc1', weights=train1_autoencoder.get_layer(\"t1tc1\").get_weights())(u1)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "\n",
    "### finetune\n",
    "\n",
    "autoencoder.fit(train_batches, train_batches, epochs=1, validation_data=(test_batches, test_batches), batch_size=bs_z, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('CAE_overhead_20ep_2pool_4f.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 't1n1/keras_learning_phase' with dtype bool\n\t [[Node: t1n1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 't1n1/keras_learning_phase', defined at:\n  File \"/home/hoffmann/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-6ed32595e320>\", line 8, in <module>\n    train1_norm1=BatchNormalization(axis=-1, name='t1n1')(input_img)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/keras/layers/normalization.py\", line 190, in call\n    training=training)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2740, in in_train_phase\n    training = learning_phase()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 121, in learning_phase\n    name='keras_learning_phase')\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1502, in placeholder\n    name=name)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2149, in _placeholder\n    name=name)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 't1n1/keras_learning_phase' with dtype bool\n\t [[Node: t1n1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 't1n1/keras_learning_phase' with dtype bool\n\t [[Node: t1n1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-95541d089bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetMaxGrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-bd32731745a5>\u001b[0m in \u001b[0;36mgetMaxGrads\u001b[0;34m(input_set, model, layer_name, nof)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0miterate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mlen1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 't1n1/keras_learning_phase' with dtype bool\n\t [[Node: t1n1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 't1n1/keras_learning_phase', defined at:\n  File \"/home/hoffmann/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-6ed32595e320>\", line 8, in <module>\n    train1_norm1=BatchNormalization(axis=-1, name='t1n1')(input_img)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/keras/layers/normalization.py\", line 190, in call\n    training=training)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2740, in in_train_phase\n    training = learning_phase()\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 121, in learning_phase\n    name='keras_learning_phase')\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1502, in placeholder\n    name=name)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2149, in _placeholder\n    name=name)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/hoffmann/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 't1n1/keras_learning_phase' with dtype bool\n\t [[Node: t1n1/keras_learning_phase = Placeholder[dtype=DT_BOOL, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "out = getMaxGrads(test_batches, autoencoder, 'c3', 8)\n",
    "\n",
    "a=len(test_batches)\n",
    "b=len(test_batches[0])\n",
    "c=len(test_batches[0][0])\n",
    "\n",
    "it_temp = np.zeros((a,b,c))\n",
    "\n",
    "for i in range(a):\n",
    "    q = np.percentile(out[i],99)\n",
    "    for j in range(b):\n",
    "        for k in range(c):\n",
    "            if(out[i][j][k]>q):\n",
    "                it_temp[i][j][k]=out[i][j][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writegrads = np.zeros((len(it_temp),len(it_temp[0])*len(it_temp[0])))\n",
    "\n",
    "for i in range(len(it_temp)):\n",
    "    temp = []\n",
    "    for j in it_temp[i]:\n",
    "        temp.extend(j)\n",
    "    writegrads[i] = temp\n",
    "\n",
    "np.savetxt('CAE_overhead_20ep_2pool_4f',writegrads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### plot\n",
    "\n",
    "decoded_imgs = autoencoder.predict(test_batches)\n",
    "show_imgs(test_batches,decoded_imgs)\n",
    "\n",
    "print(autoencoder.summary())\n",
    "\n",
    "autoencoder_show_hidden = Model(input_img, encoded)\n",
    "encoded_imgs = autoencoder_show_hidden.predict(test_batches)\n",
    "\n",
    "show_hidden(test_batches, encoded_imgs)\n",
    "\n",
    "#print grads\n",
    "\n",
    "show_imgs_gray_out(test_batches,out)\n",
    "\n",
    "show_imgs_gray_out(test_batches,it_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.count_nonzero(it_temp[0]==0)/256/256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
