{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 28, 28, 1) (5000, 28, 28, 1) (5000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "###### %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# convolutional autoencoder in keras\n",
    "\n",
    "import os\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.regularizers import l1\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "from keras.datasets import mnist \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def own_kullback_leibler_divergence(y_true, y_pred):\n",
    "    y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "    return K.sum(y_true * K.log(y_true / y_pred), axis=-1)\n",
    "\n",
    "def own_mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "def own_loss(y_true, y_pred):\n",
    "    return 1.0*own_kullback_leibler_divergence(y_true, y_pred)+own_mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# utility function for showing images\n",
    "def show_imgs(x_test, decoded_imgs, n=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        if decoded_imgs is not None:\n",
    "            ax = plt.subplot(2, n, i+ 1 +n)\n",
    "            plt.imshow(decoded_imgs[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "### Create input\n",
    "\n",
    "(x_all, y_all), (x_all_test, y_all_test) = mnist.load_data()\n",
    "\n",
    "x_all = x_all.astype('float32')/255. # 0-1.に変換\n",
    "x_all_test = x_all_test.astype('float32')/255.\n",
    "\n",
    "###\n",
    "\n",
    "x_9_list=[]\n",
    "for i in range(len(y_all)):\n",
    "    if (y_all[i] == 9):\n",
    "        x_9_list.append(x_all[i])\n",
    "x_9=np.asarray(x_9_list[:5000])\n",
    "x_9_list_test=[]\n",
    "for i in range(len(y_all_test)):\n",
    "    if (y_all_test[i] == 9):\n",
    "        x_9_list_test.append(x_all_test[i])\n",
    "x_9_test=np.asarray(x_9_list_test[:500])\n",
    "\n",
    "###\n",
    "\n",
    "x_6_list=[]\n",
    "for i in range(len(y_all)):\n",
    "    if (y_all[i] == 6):\n",
    "        x_6_list.append(x_all[i])\n",
    "x_6=np.asarray(x_6_list[:5000])\n",
    "x_6_list_test=[]\n",
    "for i in range(len(y_all_test)):\n",
    "    if (y_all_test[i] == 6):\n",
    "        x_6_list_test.append(x_all_test[i])\n",
    "x_6_test=np.asarray(x_6_list_test[:500])\n",
    "\n",
    "###\n",
    "\n",
    "x_8_list=[]\n",
    "for i in range(len(y_all)):\n",
    "    if (y_all[i] == 8):\n",
    "        x_8_list.append(x_all[i])\n",
    "x_8=np.asarray(x_8_list[:5000])\n",
    "x_8_list_test=[]\n",
    "for i in range(len(y_all_test)):\n",
    "    if (y_all_test[i] == 8):\n",
    "        x_8_list_test.append(x_all_test[i])\n",
    "x_8_test=np.asarray(x_8_list_test[:500])\n",
    "\n",
    "###\n",
    "\n",
    "x_9 = np.reshape(x_9, (len(x_9), 28, 28, 1))\n",
    "x_9_test = np.reshape(x_9_test, (len(x_9_test), 28, 28, 1))\n",
    "\n",
    "x_6 = np.reshape(x_6, (len(x_6), 28, 28, 1))\n",
    "x_6_test = np.reshape(x_6_test, (len(x_6_test), 28, 28, 1))\n",
    "\n",
    "x_8 = np.reshape(x_8, (len(x_8), 28, 28, 1))\n",
    "x_8_test = np.reshape(x_8_test, (len(x_8_test), 28, 28, 1))\n",
    "\n",
    "print(x_9.shape,x_6.shape,x_8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 4s 760us/step - loss: 0.0879 - val_loss: -0.0075\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 4s 714us/step - loss: -0.0084 - val_loss: -0.0100\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 4s 713us/step - loss: -0.0098 - val_loss: -0.0109\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 4s 736us/step - loss: -0.0104 - val_loss: -0.0101\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 4s 716us/step - loss: -0.0107 - val_loss: -0.0096\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 4s 717us/step - loss: -0.0110 - val_loss: -0.0116\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 4s 717us/step - loss: -0.0111 - val_loss: -0.0117\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 4s 745us/step - loss: -0.0112 - val_loss: -0.0117\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 4s 721us/step - loss: -0.0113 - val_loss: -0.0115\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 4s 719us/step - loss: -0.0114 - val_loss: -0.0119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x134a63c8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### pretrain1\n",
    "    \n",
    "input_img = Input(shape=(28, 28,1)) # 1ch=black&white, 28 x 28\n",
    "\n",
    "train1_norm=BatchNormalization(axis=-1, name='t1n')(input_img)\n",
    "train1_c1 = Conv2D(16, (3, 3), activation='relu', padding='same', name='t1c1')(train1_norm)\n",
    "train1_decoded = Conv2DTranspose(1, (3, 3), activation='relu', padding='same', name='t1tc1')(train1_c1)\n",
    "\n",
    "train1_autoencoder = Model(input_img, train1_decoded)\n",
    "train1_autoencoder.compile(optimizer='adadelta', loss=own_loss)\n",
    "\n",
    "train1_autoencoder.fit(x_9, x_9, epochs=10, batch_size=20,\n",
    "               shuffle=True, validation_data=(x_9_test, x_9_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "[[[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True]]\n",
      "\n",
      " [[ True  True  True  True  True  True  True  True  True  True  True\n",
      "    True  True  True  True  True]]]\n",
      "True\n",
      "(5000, 28, 28, 16)\n"
     ]
    }
   ],
   "source": [
    "### pretrain2\n",
    "\n",
    "creator_train2 = Model(input_img, train1_c1)\n",
    "\n",
    "a = creator_train2.get_weights()\n",
    "\n",
    "creator_train2.get_layer(\"t1c1\").set_weights(train1_autoencoder.get_layer('t1c1').get_weights())\n",
    "\n",
    "b = creator_train2.get_weights()\n",
    "\n",
    "\n",
    "for i in range(len(a)):\n",
    "    for j in range(len(a[0])):\n",
    "        print(a[i][j]==b[i][j])\n",
    "\n",
    "        \n",
    "'''\n",
    "test_input_img = Input(shape=(28, 28,1)) # 1ch=black&white, 28 x 28\n",
    "\n",
    "test_train1_norm=BatchNormalization(axis=-1, name='t1n')(input_img)\n",
    "test_train1_c1 = Conv2D(16, (3, 3), activation='relu', padding='same', name='t1c1',\n",
    "                        kernel_initializer=train1_autoencoder.get_layer('t1c1').get_weights()[0],\n",
    "                        bias_initializer=train1_autoencoder.get_layer('t1c1').get_weights()[1])(train1_norm)\n",
    "test_train1_decoded = Conv2DTranspose(1, (3, 3), activation='relu', padding='same', name='t1tc1')(train1_c1)\n",
    "\n",
    "test_train1_autoencoder = Model(input_img, train1_decoded)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "creator_train2 = Model(input_img, train1_c1)\n",
    "creator_train2.get_layer(\"t1c1\").set_weights(train1_autoencoder.get_layer('t1c1').get_weights())\n",
    "\n",
    "input_train2 = creator_train2.predict(x_9)\n",
    "\n",
    "print(input_train2.shape)\n",
    "\n",
    "#---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img_train2 = Input(shape=(28, 28, 16))\n",
    "train2_c1 = Conv2D(8, (3, 3), activation='relu', padding='same', name='t2c1')(input_img_train2)\n",
    "train2_decoded = Conv2DTranspose(16, (3, 3), activation='relu', padding='same', name='t2tc1')(train2_c1)\n",
    "\n",
    "train2_autoencoder = Model(input_img_train2, train2_decoded)\n",
    "train2_autoencoder.compile(optimizer='adadelta', loss=own_loss)\n",
    "\n",
    "train2_autoencoder.fit(input_train2, input_train2, epochs=10, batch_size=20, verbose=1)\n",
    "\n",
    "### pretrain3\n",
    "\n",
    "creator_train3 = Model(input_img_train2, train2_c1)\n",
    "creator_train3.get_layer(\"t2c1\").set_weights(train2_autoencoder.get_layer('t2c1').get_weights())\n",
    "\n",
    "input_train3 = creator_train3.predict(input_train2)\n",
    "\n",
    "print(input_train2.shape)\n",
    "\n",
    "#---\n",
    "\n",
    "input_img_train3 = Input(shape=(28, 28, 8))\n",
    "train3_p1 = MaxPooling2D(pool_size=(2, 2), name='t3p1')(input_img_train3)\n",
    "train3_c1 = Conv2D(4, (3, 3), activation='relu', padding='same', name='t3c1')(train3_p1)\n",
    "train3_tc1 = Conv2DTranspose(8, (3, 3), activation='relu', padding='same', name='t3tc1')(train3_c1)\n",
    "train3_decoded = UpSampling2D(size=(2, 2), name='t3u1')(train3_tc1)\n",
    "\n",
    "train3_autoencoder = Model(input_img_train3, train3_decoded)\n",
    "train3_autoencoder.compile(optimizer='adadelta', loss=own_loss)\n",
    "\n",
    "train3_autoencoder.fit(input_train3, input_train3, epochs=10, batch_size=20, verbose=1)\n",
    "\n",
    "### unroll\n",
    "\n",
    "input_img = Input(shape=(28, 28,1)) # 1ch=black&white, 28 x 28\n",
    "\n",
    "norm=BatchNormalization(axis=-1, name='n')(input_img)\n",
    "c1 = Conv2D(16, (3, 3), activation='relu', padding='same', name='c1')(norm)\n",
    "c2 = Conv2D(8, (3, 3), activation='relu', padding='same', name='c2')(c1)\n",
    "p1 = MaxPooling2D(pool_size=(2, 2), name='p1')(c2)\n",
    "encoded = Conv2D(4, (3, 3), activation='relu', padding='same', name='c3')(p1)\n",
    "tc3 = Conv2DTranspose(8, (3, 3), activation='relu', padding='same', name='tc3')(encoded)\n",
    "u1 = UpSampling2D(size=(2, 2), name='u1')(tc3)\n",
    "tc2 = Conv2DTranspose(16, (3, 3), activation='relu', padding='same', name='tc2')(u1)\n",
    "decoded = Conv2DTranspose(1, (3, 3), activation='relu', padding='same', name='tc1')(tc2)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss=own_loss)\n",
    "\n",
    "autoencoder.get_layer(\"c1\").set_weights(train1_autoencoder.get_layer('t1c1').get_weights())\n",
    "autoencoder.get_layer(\"c2\").set_weights(train2_autoencoder.get_layer('t2c1').get_weights())\n",
    "autoencoder.get_layer(\"c3\").set_weights(train3_autoencoder.get_layer('t3c1').get_weights())\n",
    "autoencoder.get_layer(\"tc3\").set_weights(train3_autoencoder.get_layer('t3tc1').get_weights())\n",
    "autoencoder.get_layer(\"tc2\").set_weights(train2_autoencoder.get_layer('t2tc1').get_weights())\n",
    "autoencoder.get_layer(\"tc1\").set_weights(train1_autoencoder.get_layer('t1tc1').get_weights())\n",
    "\n",
    "### finetune\n",
    "\n",
    "autoencoder.fit(x_9, x_9, epochs=10, batch_size=20,\n",
    "            shuffle=True, validation_data=(x_9_test, x_9_test), verbose=1)\n",
    "\n",
    "### plot\n",
    "\n",
    "decoded_imgs = autoencoder.predict(x_9_test)\n",
    "show_imgs(x_9_test,decoded_imgs)\n",
    "\n",
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enbig1 = UpSampling2D(size=(2, 2))(encoded)\n",
    "#enbig2 = UpSampling2D(size=(2, 2))(enbig1)\n",
    "\n",
    "autoencoder_show_hidden = Model(input_img, enbig1)\n",
    "encoded_imgs = autoencoder_show_hidden.predict(x_9_test)\n",
    "\n",
    "def show_hidden(x_test, encoded_imgs, n=10):\n",
    "    \n",
    "    en0=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "    en1=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "    en2=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "    en3=np.zeros((encoded_imgs.shape[0],encoded_imgs.shape[1],encoded_imgs.shape[2]))\n",
    "\n",
    "    for i in range(encoded_imgs.shape[0]):\n",
    "        for j in range(encoded_imgs.shape[1]):\n",
    "            for k in range(encoded_imgs.shape[2]):\n",
    "                en0[i][j][k]=encoded_imgs[i][j][k][0]\n",
    "                en1[i][j][k]=encoded_imgs[i][j][k][1]\n",
    "                en2[i][j][k]=encoded_imgs[i][j][k][2]\n",
    "                en3[i][j][k]=encoded_imgs[i][j][k][3]\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(5, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        if decoded_imgs is not None:\n",
    "            ax = plt.subplot(5, n, i+ 1 +n)\n",
    "            plt.imshow(en0[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "            ax = plt.subplot(5, n, i+ 1 +2*n)\n",
    "            plt.imshow(en1[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "            ax = plt.subplot(5, n, i+ 1 +3*n)\n",
    "            plt.imshow(en2[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "            ax = plt.subplot(5, n, i+ 1 +4*n)\n",
    "            plt.imshow(en3[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "show_hidden(x_9_test, encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
